{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4729d7-45e3-4862-a820-49997a9a90e3",
   "metadata": {},
   "source": [
    "## 🔎크롤링\n",
    "\n",
    "### **BeautifulSoup4**\n",
    "- 파이썬에서 HTML 및 XML 문서를 파싱하여 원하는 데이터를 추출할 수 있도록 도와주는 라이브러리\n",
    "- 웹 크롤링이나 스크래핑 작업에 자주 사용, requests와 함께 사용하면 웹 페이지의 정보를 손쉽게 가져올 수 있다.\n",
    "\n",
    "#### 1. **설치**\n",
    "```bash\n",
    "# beautifulsoup 설치\n",
    "pip install beautifulsoup4\n",
    "\n",
    "# requests - 웹 페이지 요청 라이브러리\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "#### 2. **기본 사용법**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')  # 또는 'lxml'\n",
    "    print(soup.prettify())  # HTML 구조를 보기 좋게 출력\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### 3. **주요 메서드**\n",
    "**3-1. 태그 이름으로 요소 찾기**\n",
    "```python\n",
    "# <title> 태그 전체\n",
    "soup.title  \n",
    "# <title> 태그의 텍스트만\n",
    "soup.title.string  \n",
    "# 첫 번째 <a> 태그\n",
    "soup.a \n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**3-2. `find()`와 `find_all()`**\n",
    "```python\n",
    "# 첫 번째 <a> 태그\n",
    "soup.find('a')  \n",
    "# class가 'link'인 첫 번째 <a> 태그\n",
    "soup.find('a', class_='link')  \n",
    "# class로 선택시 class_ 라고 명시하지 않고 생략 가능\n",
    "soup.find('a', 'link')  \n",
    "# attrs 딕셔너리 이용하여 여러 속성을 동시에 지정하기\n",
    "soup.find('a', attrs={'id' : 'id_name', class_ : 'link'})\n",
    "# id값으로 찾기\n",
    "soup.find(id='id_name')\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# 모든 <a> 태그 리스트 형태로 반환\n",
    "soup.find_all('a') \n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# find()로 크게 감싸는 html을 추출하고 다시 추출된 데이터에서 원하는 부분을 find_all()로 추출 가능\n",
    "soup.find(id='id_name').find_all('a')\n",
    "\n",
    "```\n",
    "**💡크롤링 시 `class`로 많이 선택한다**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**3-3.CSS 선택자 사용 `select()` & `selet_one()`**\n",
    "```python\n",
    "# class가 'classname'인 모든 <div> 태그\n",
    "soup.select('div.classname')  \n",
    "  # id가 'id'인 첫 번째 태그\n",
    "soup.select_one('#id')\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**3-4.텍스트 및 속성 추출**\n",
    "```python\n",
    "tag = soup.find('a')\n",
    "\n",
    "# 태그 내부의 텍스트\n",
    "tag.get_text()\n",
    "\n",
    "# get() 메서드 -> 속성이 존재하지 않을 경우 `None`을 반환하므로 상대적으로 더 안전하게 속성값 추출 가능\n",
    "href = tag.get('href')\n",
    "href = tag.get('title')\n",
    "\n",
    "# href 속성 값 -> 존재하지 않을 경우 `KeyError`를 발생시킴\n",
    "tag['href']\n",
    "tag['title']\n",
    "\n",
    "# 모든 속성 딕셔너리\n",
    "tag.attrs  # {'href' : 'https://example.com', 'title' : 'Ex'}\n",
    "\n",
    "```\n",
    "\n",
    "**3-5.그 외**\n",
    "```python\n",
    "# 특정 속성 존재 확인 : has_attr()\n",
    "if tag.has_attr('href') :\n",
    "    print('링크가 존재합니다')\n",
    "```\n",
    "\n",
    "[**💡공식문서**](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73313db7-58b1-4dda-a02c-388109c4059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<부산 광안리서 고령 운전자 차량 인도 돌진… 보행자 5명 부상>\n",
      "<‘한동훈 공격사주 논란’ 김대남 전 행정관, 민주당 선대위 합류>\n",
      "<[단독] \"건진법사, 김 여사에 전달하려 선물 받은 것\">\n",
      "<[21대 대선] 김용태 \"김건희 문제, 진심어린 반성과 사과\">\n",
      "<이재명이 쏜 '호텔경제론' 논란…진보 학자도 \"현실성 없는 우화\">\n",
      "<‘역성장’ 먹구름 드리운 한국 경제>\n",
      "<김종인 \"한동훈 유세 자세, 과거 이명박 후보 때 박근혜 대표와 비슷해\">\n",
      "<“모건스탠리가?” 다이먼 JP모건 CEO 비트코인 구매 허용한 까닭은>\n",
      "<[속보] ‘선관위서 中 간첩 99명 체포’ 보도 기자 구속영장 기각>\n",
      "<‘중국 간첩 99명’ 보도 스카이데일리 기자 구속영장 기각>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://news.naver.com/'\n",
    "res = requests.get(url)\n",
    "title_li = list()\n",
    "\n",
    "if res.status_code == 200 :\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    for text in soup.find_all('strong',class_='cnf_news_title', limit=10) :\n",
    "        title_li.append(text.get_text())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "for title in title_li :\n",
    "    print(f\"<{title}>\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5177421-65a2-4db1-86a7-9ee342178a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 연습 #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "439106dd-0cc6-4e73-b64a-600f4af297ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[나만의 엣지있는 블로그 사이트 만들기]\n",
      "(왕초보) - 클래스 소개 -> 링크 : https://www.fun-coding.org\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기 -> 링크 : https://www.fun-coding.org\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기 -> 링크 : https://www.fun-coding.org\n",
      "(왕초보) - 초간단 페이지 만들어보기 -> 링크 : https://www.fun-coding.org\n",
      "(왕초보) - 이쁘게 테마 적용해보기 -> 링크 : https://www.fun-coding.org\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기 -> 링크 : https://www.fun-coding.org\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기 -> 링크 : https://www.fun-coding.org\n",
      "\n",
      "\n",
      "[당신의 커리어에 파이썬을 입히세요! 자신만의 자동 프로그램까지 가져가는 특별한 강의]\n",
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2] -> 링크 : https://www.fun-coding.org\n",
      "(초급) - 필요한 프로그램 설치 시연 [5] -> 링크 : https://www.fun-coding.org\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9] -> 링크 : https://www.fun-coding.org\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8] -> 링크 : https://www.fun-coding.org\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7] -> 링크 : https://www.fun-coding.org\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40] -> 링크 : https://www.fun-coding.org\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12] -> 링크 : https://www.fun-coding.org\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42] -> 링크 : https://www.fun-coding.org\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412] -> 링크 : https://www.fun-coding.org\n"
     ]
    }
   ],
   "source": [
    "url2 = 'https://davelee-fun.github.io/blog/crawl_test'\n",
    "res2 = requests.get(url2)\n",
    "\n",
    "if res2.status_code == 200 :\n",
    "    soup = BeautifulSoup(res2.content, 'html.parser')\n",
    "    hobby_list = soup.find('ul', id='hobby_course_list').find_all('li','course')\n",
    "                           \n",
    "    dev_list = soup.find('ul', id='dev_course_list').find_all('li','course')\n",
    "\n",
    "    print(f\"[{soup.select('h3')[0].string}]\")\n",
    "    for elem in hobby_list :\n",
    "        ancher = elem.find('a')\n",
    "        print(f\"{ancher.string} -> 링크 : {ancher.get('href')}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(f\"[{soup.select('h3')[1].string}]\")\n",
    "    for elem in dev_list :\n",
    "        ancher = elem.find('a')\n",
    "        print(f\"{ancher.string} -> 링크 : {ancher.get('href')}\")\n",
    "    \n",
    "        \n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81fc71-d355-4a78-98a3-6bbf7a0fe9b7",
   "metadata": {},
   "source": [
    "## 🧹 크롤링 데이터 전처리에 자주 사용되는 Python 함수\n",
    "\n",
    "### 1. 문자열 처리 함수\n",
    "\n",
    "* **`strip()`**: 문자열의 양쪽 공백을 제거.\n",
    "* **`split()`**: 지정한 구분자를 기준으로 문자열을 나눔.\n",
    "* **`replace()`**: 특정 문자열을 다른 문자열로 대체.\n",
    "* **`lower()` / `upper()`**: 문자열을 소문자 또는 대문자로 변환.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```python\n",
    "text = \"  Hello, World!  \"\n",
    "clean_text = text.strip().lower().replace(\",\", \"\")\n",
    "print(clean_text)  # 출력: hello world!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. 정규 표현식 (`re` 모듈)\n",
    "\n",
    "복잡한 패턴의 문자열을 처리할 때 유용.\n",
    "\n",
    "* **`re.sub()`**: 정규식을 활용하여 문자열을 대체.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"Contact us at support@example.com\"\n",
    "clean_text = re.sub(r'\\S+@\\S+', '', text)\n",
    "print(clean_text)  # 출력: Contact us at\n",
    "```\n",
    "\n",
    "\n",
    "### 3. HTML 태그 제거\n",
    "\n",
    "크롤링한 데이터에서 HTML 태그를 제거하여 순수한 텍스트만 추출가능.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"<p>This is a <b>test</b>.</p>\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "print(text)  # 출력: This is a test.\n",
    "```\n",
    "\n",
    "### 4. 불용어 제거\n",
    "\n",
    "자연어 처리에서 의미 없는 단어(예: \"the\", \"is\")를 제거하여 분석의 정확도를 높임.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```python\n",
    "stopwords = ['the', 'is', 'at', 'which', 'on']\n",
    "words = \"The cat is on the mat.\".lower().split()\n",
    "filtered_words = [word for word in words if word not in stopwords]\n",
    "print(filtered_words)  # 출력: ['cat', 'mat.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 `enumerate()` 함수의 기본 사용법\n",
    "\n",
    "`enumerate()` 함수는 두 개의 인자를 받는다 :\n",
    "\n",
    "* **iterable**: 리스트, 튜플, 문자열 등 반복 가능한 객체\n",
    "* **start** (선택적): 인덱스의 시작 값 (기본값은 0)\n",
    "\n",
    "함수는 각 요소와 해당 인덱스를 포함하는 튜플을 생성하는 이터레이터를 반환\n",
    "\n",
    "### ✅ 기본 예제\n",
    "\n",
    "```python\n",
    "fruits = ['apple', 'banana', 'cherry']\n",
    "for index, fruit in enumerate(fruits):\n",
    "    print(f\"{index}: {fruit}\")\n",
    "```\n",
    "\n",
    "**출력:**\n",
    "\n",
    "```\n",
    "0: apple\n",
    "1: banana\n",
    "2: cherry\n",
    "```\n",
    "\n",
    "### ✅ 시작 인덱스 지정\n",
    "\n",
    "```python\n",
    "for index, fruit in enumerate(fruits, start=1):\n",
    "    print(f\"{index}: {fruit}\")\n",
    "```\n",
    "\n",
    "**출력:**\n",
    "\n",
    "```\n",
    "1: apple\n",
    "2: banana\n",
    "3: cherry\n",
    "```\n",
    "\n",
    "이처럼 `start` 매개변수를 사용하면 인덱스의 시작 값을 지정 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bb88e-f694-4e62-8a05-67144e5935b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e8107-06ae-45c1-b257-7c9490a7a77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95de93-6b71-48af-a8e4-e549ba90d1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2945d78e-b3b9-4973-bc8e-329af9fe9f83",
   "metadata": {},
   "source": [
    "### 🔖 참고(인용)\n",
    "> [인프런: 파이썬으로 크롤링 시작하기 - 기본편](https://www.inflearn.com/course/python-crawling-basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2181d6-259f-4985-b08b-f86fd5c96c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
